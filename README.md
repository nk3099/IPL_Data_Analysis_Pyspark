# IPL_Data_Analysis_Pyspark

##Overview of the Design

In this blog post, we will understand how to perform simple operations on top of a relational database to get valuable Insights using Apache Spark.

##Problem Statement:
We have five data tables which are of type CSV. We are performing basic joins in those tables and Creating a Demoralized data frame so that we could perform some processing and Analytics on top of our Data.

##Dataset:
For this project, we are going to use the IPL Dataset. These Datasets consist of three tables customer, orders, and order_items.

##Tech Stack / Skill used:
Spark
SQL

##Setting the workspace:
We will use Databricks Community Edition which is free of cost to perform the Spark operations if you prefer to use spark locally or in the Hadoop cluster itâ€™s fine.

Refer: https://www.databricks.com/product/faq/community-edition

After setting up the workspace create a cluster and Open a workbook. You are all set to go.

Project:
Now it's time to add our data to the Databricks.
